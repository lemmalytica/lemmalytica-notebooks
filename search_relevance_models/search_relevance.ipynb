{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.10 64-bit ('nlp': conda)",
   "display_name": "Python 3.6.10 64-bit ('nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a38b35b77e6756a055eaa8a5c714418c79b2c1847796cd1ba287d8719b86b0a5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introduction to Search Relevance Models\n",
    "\n",
    "*Note: This is the companion notebook to the [Introduction to Search Relevance Models](https://lemmalytica.com/posts/2020/10/13/search-relevance-models/) article on [Lemmalytica](https://lemmalytica.com).*\n",
    "\n",
    "One of the core tasks in information retrieval is searching. Anyone who deals with large amounts of text data (and that's almost all of us) knows how difficult this seemingly simple task can be. If your search term is too broad, you may find yourself sifting through an impossible quantity of documents. And if your search term is too narrow, you are likely to miss out on important documents. So how do we decide which documents are most relevant to our search?\n",
    "\n",
    "Search relevance is a difficult problem--and modern search engines employ highly sophisticated (and proprietary) algorithms to deal with the issue. We won't delve into those algorithms, but let's look at some simple strategies that you might employ in your own information retrieval applications."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## State of the Union\n",
    "\n",
    "As a sample dataset, we will be using the text of State of the Union (SotU) addresses by U.S. Presidents. Imagine that you're a historian studying different policy priorities in U.S. history. How would you decide which SotU address was most related to your topic of interest? For example, say you are interested in taxes. Is a simple search in the corpus for \"taxes\" enough? Will that lead to too many results and information overload? Let's find out!\n",
    "\n",
    "To get started, we will load the libraries we are going to use and read our data into a dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility and data libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import NLP libraries\n",
    "import spacy\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load state of the union texts\n",
    "def load_texts(dir_path):\n",
    "    \"\"\"\n",
    "    - Parameters: dir_path (string) for a directory containing text\n",
    "      files.\n",
    "    - Returns: A list of dictionaries with keys file_name and text.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if file_name.endswith(\".txt\") and os.path.isfile(file_path):\n",
    "            with open(file_path, \"r+\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                current = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"text\": text\n",
    "                }\n",
    "                docs.append(current)\n",
    "    return docs\n",
    "\n",
    "def add_sotu_metadata(sotu_doc_dict):\n",
    "    \"\"\"\n",
    "    - Parameters: sotu_doc_dict (dictionary) with sotu metadata.\n",
    "      Expects a file_name key in format \"president_year.txt\"\n",
    "    - Returns: A dictionary with appended president and year keys.\n",
    "    \"\"\"\n",
    "    file_name = sotu_doc_dict[\"file_name\"]\n",
    "    pres, year, filetype = re.split(r\"[^A-Za-z0-9]\", file_name)\n",
    "    sotu_doc_dict[\"president\"] = pres\n",
    "    sotu_doc_dict[\"year\"] = int(year)\n",
    "    return sotu_doc_dict\n",
    "\n",
    "def load_sotu_texts(dir_path):\n",
    "    \"\"\"\n",
    "    - Parameters: dir_path (string) for a directory containing text\n",
    "      files.\n",
    "      Expects sotu text files in dir_path in format \"president_year.txt\".\n",
    "    - Returns: A Pandas DataFrame with file_name, text, president, and\n",
    "      year columns for each sotu text in dir_path.\n",
    "    \"\"\"\n",
    "    docs = load_texts(dir_path)\n",
    "    docs = [add_sotu_metadata(d) for d in docs]\n",
    "    docs = sorted(docs, key=lambda d: d[\"year\"])\n",
    "    df = pd.DataFrame(docs)\n",
    "    return df[[\"year\", \"president\", \"text\"]]\n",
    "\n",
    "sotu_df = load_sotu_texts(\"data\")"
   ]
  },
  {
   "source": [
    "### Exploring Data\n",
    "\n",
    "As with any data project, we should start by looking at our data and seeing what we can learn about it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total rows: 228\nDate range: 1790 to 2018\nInitial rows:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   year   president                                               text\n0  1790  Washington  Fellow Citizens of the Senate, and House of Re...\n1  1791  Washington  Fellow-Citizens of the Senate and House of Rep...\n2  1792  Washington  Fellow-Citizens of the Senate and House of Rep...\n3  1793  Washington  Fellow-Citizens of the Senate and House of Rep...\n4  1794  Washington  Fellow-Citizens of the Senate and House of Rep...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>president</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1790</td>\n      <td>Washington</td>\n      <td>Fellow Citizens of the Senate, and House of Re...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1791</td>\n      <td>Washington</td>\n      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1792</td>\n      <td>Washington</td>\n      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1793</td>\n      <td>Washington</td>\n      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1794</td>\n      <td>Washington</td>\n      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "print(f\"Total rows: {sotu_df.shape[0]}\")\n",
    "print(f\"Date range: {min(sotu_df['year'])} to {max(sotu_df['year'])}\")\n",
    "print(\"Initial rows:\")\n",
    "display(sotu_df.head())\n"
   ]
  },
  {
   "source": [
    "It looks like we have 228 observations (that is, different SotU speeches), each with `year`, `president`, and `text` columns. Also, it seems like Washington liked to start all of his speeches the exact same way!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simple Search\n",
    "\n",
    "Now that we have our data, let's start searching for specific terms. At its most basic, a search query asks the question \"which documents contain all of the terms in this query?\" This is a simple strategy and we can use it to search through the `text` column of our dataframe. \n",
    "\n",
    "Putting on our historian hat, let's imagine that we are interested in comparing technology and tax policy. As part of our work, we want to find all of the SotU addresses that discuss both issues. To do this, we will use the search query \"technology taxes\". For the sake of simplicity, we will treat this as a boolean `and` query.\n",
    "\n",
    "First, we will need to split our query into individual tokens and then filter our dataframe for all of the observations where the `text` column has at least one occurrence of each of the terms in our query."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num results for query 'technology taxes': 38\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     year   president                                               text\n",
       "116  1906   Roosevelt  To the Senate and House of Representatives:\\n\\...\n",
       "165  1956  Eisenhower  To the Congress of the United States:\\n\\nThe o...\n",
       "170  1961  Eisenhower  To the Congress of the United States:\\n\\nOnce ...\n",
       "174  1965     Johnson  On this Hill which was my home, I am stirred b...\n",
       "176  1967     Johnson  Mr. Speaker, Mr. Vice President, distinguished..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>president</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>116</th>\n      <td>1906</td>\n      <td>Roosevelt</td>\n      <td>To the Senate and House of Representatives:\\n\\...</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>1956</td>\n      <td>Eisenhower</td>\n      <td>To the Congress of the United States:\\n\\nThe o...</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>1961</td>\n      <td>Eisenhower</td>\n      <td>To the Congress of the United States:\\n\\nOnce ...</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>1965</td>\n      <td>Johnson</td>\n      <td>On this Hill which was my home, I am stirred b...</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>1967</td>\n      <td>Johnson</td>\n      <td>Mr. Speaker, Mr. Vice President, distinguished...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# simple boolean search\n",
    "def search_df_texts(df, query_string: str):\n",
    "    \"\"\"\n",
    "    - Parameters: df (Pandas DataFrame), query_string (string). df must \n",
    "      contain a \"text\" column.\n",
    "    - Returns: A subset of df containing only rows where each term in \n",
    "      query_string appeared as a substring in df[\"text\"].\n",
    "    \"\"\"\n",
    "    terms = query_string.lower().split(\" \")\n",
    "    filters = [df[\"text\"].str.lower().str.contains(term)\n",
    "               for term in terms]\n",
    "    return df[np.all(filters, axis=0)]\n",
    "\n",
    "search_term = \"technology taxes\"\n",
    "results = search_df_texts(sotu_df, search_term)\n",
    "\n",
    "print(f\"Num results for query '{search_term}': {results.shape[0]}\\n\")\n",
    "results.head()"
   ]
  },
  {
   "source": [
    "Great! We have 38 results that include both the term `technology` and the term `taxes`. But how do we know where to start? Our dataframe was already sorted in chronological order, which is also how the search results are sorted. However, this tells us nothing about which of the SotU speeches is *most* relevant to our search. We're going to have to read through all 38 speeches to see which ones are the most useful. There must be a better way!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Bag-of-Words and the Term-Document Matrix\n",
    "\n",
    "To improve on our simple search model, we need some kind of metric to use when comparing search results. We already know that all of our search results contain each of our search terms at least once, so what else can we do? The most obvious solution is to look at word frequency. If one document contains our search terms many times, and another document just a few times, then the first one is likely more relevant.\n",
    "\n",
    "To look at word frequency, we can use the so-called [*bag-of-words*](https://lemmalytica.com/posts/2020/10/11/bag-of-words/) (BoW) model. A BoW turns a document into a collection of token-frequency tuples. We can view this collection as a vectorized version of our text, which is useful in various machine learning tasks. For our purposes though, we aren't feeding the model into an algorithm--rather, we want to use it as a metric when comparing search results.\n",
    "\n",
    "To use the BoW as a search relevance metric, we can convert it into a `term-document matrix`. A term-document matrix is a table where each row is a document and each column is a token. The values in the cells are the number of times a particular token occurs in each document. Using this, we can sort our search results by the count of a particular token (or the sum of the counts of all the terms in our search)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Preparation\n",
    "\n",
    "To get started, we will need to tokenize our documents and build a *vocabulary* from the overall collection. A vocabulary is the set of unique tokens in our text and will be used as the columns in our term-document matrix. To carry out these tasks, we can use [*spaCy*](https://spacy.io/) for tokenization and [*Gensim*](https://radimrehurek.com/gensim/index.html) to build a vocabulary (aka *dictionary*)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# tokenize documents\n",
    "def spacy_doc(model, text, lower=True):\n",
    "    \"\"\"\n",
    "    - Parameters: model (spaCy model), text (string), lower (bool).\n",
    "    - Returns: A spaCy Document object processed using the provided\n",
    "      model. Document is all lowercase if lower is True.\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    return model(text)\n",
    "\n",
    "sotu_docs = [spacy_doc(nlp, text) for text in sotu_df[\"text\"]]\n",
    "\n",
    "# build dictionary\n",
    "def get_token_texts(doc):\n",
    "    \"\"\"\n",
    "    - Parameters: doc (spaCy Document object).\n",
    "    - Returns: A list of strings based on the text value of each token\n",
    "      in doc.\n",
    "    \"\"\"\n",
    "    token_list = [token for token in doc]\n",
    "    return [token.text for token in token_list]\n",
    "\n",
    "def build_dictionary(doc_list):\n",
    "    \"\"\"\n",
    "    - Parameters: doc_list (list of spaCy Document objects).\n",
    "    - Returns: A Gensim Dictionary, built using the tokens in each\n",
    "      document contained in doc_list.\n",
    "    \"\"\"\n",
    "    return Dictionary([get_token_texts(doc) for doc in doc_list])\n",
    "\n",
    "sotu_dictionary = build_dictionary(sotu_docs)"
   ]
  },
  {
   "source": [
    "### Building a Term-Document Matrix\n",
    "\n",
    "Now that we have our tokenized documents and our corpus vocabulary, we can build our BoW representations for each text and use those to create term-document matrices. Of note, Gensim `Dictionary` objects represent each token by a numerical id rather than the token text. Since we're interested in manually inspecting our term-document matrices, we should re-label the columns with the actual token texts and convert the matrix into a dataframe for easy manipulation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bag-of-words representations and a term-document matrix\n",
    "def build_corpus(doc_list, dictionary):\n",
    "    \"\"\"\n",
    "    - Parameters: doc_list (list of spaCy Document objects), dictionary\n",
    "      (Gensim Dictionary object).\n",
    "    - Returns: A list of documents in bag-of-words format, containing\n",
    "      tuples with (token_id, token_count) for each token in the text.\n",
    "    \"\"\"\n",
    "    return [dictionary.doc2bow(get_token_texts(doc)) for doc in doc_list]\n",
    "\n",
    "def build_td_matrix(doc_list, dictionary):\n",
    "    \"\"\"\n",
    "    - Parameters: doc_list (list of spaCy Document objects), dictionary\n",
    "      (Gensim Dictionary object).\n",
    "    - Returns: A term-document matrix in the form of a 2D NumPy Array,\n",
    "      where each row contains the count of a token in the corresponding\n",
    "      document and each column index is the id of a token in the\n",
    "      dictionary.\n",
    "    \"\"\"\n",
    "    corpus = build_corpus(sotu_docs, sotu_dictionary)\n",
    "    tdm = []\n",
    "    for bow in corpus:\n",
    "        vector = np.zeros(len(dictionary))\n",
    "        for token_id, token_count in bow:\n",
    "            vector[token_id] = token_count\n",
    "        tdm.append(vector)\n",
    "    return np.array(tdm)\n",
    "\n",
    "def build_term_document_df(doc_list, dictionary):\n",
    "    \"\"\"\n",
    "    - Parameters: doc_list (list of spaCy Document objects), dictionary\n",
    "      (Gensim Dictionary object).\n",
    "    - Returns a term-document matrix in the form of a Pandas Dataframe, \n",
    "      where each row is a document and each column is a token. Values in\n",
    "      the dataframe are token counts for the given document / token.\n",
    "    \"\"\"\n",
    "    tdm = build_td_matrix(doc_list, dictionary)\n",
    "    cols = list(dictionary.token2id.keys())\n",
    "    return pd.DataFrame(tdm, columns=cols, dtype=pd.Int64Dtype)\n",
    "\n",
    "sotu_td_df = build_term_document_df(sotu_docs, sotu_dictionary)"
   ]
  },
  {
   "source": [
    "### Exploring the Term-Document Matrix\n",
    "\n",
    "Per usual, now that we have a new dataframe we had better take a look at it!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of documents: 228\nNumber of terms: 28393\n\nSample observations:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  contributes convenience convey convincing cool\n",
       "0           1           1      1          1    1\n",
       "1           1           2      0          0    0\n",
       "2           0           0      0          0    0\n",
       "3           0           1      0          0    0\n",
       "4           0           1      0          0    0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contributes</th>\n      <th>convenience</th>\n      <th>convey</th>\n      <th>convincing</th>\n      <th>cool</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "print(f\"Number of documents: {sotu_td_df.shape[0]}\")\n",
    "print(f\"Number of terms: {sotu_td_df.shape[1]}\\n\")\n",
    "print(f\"Sample observations:\")\n",
    "sotu_td_df.iloc[:5, 100:105]"
   ]
  },
  {
   "source": [
    "As expected, we still have 228 documents. Each document is an observation (row) in the matrix and has 28,393 columns. Wow! That's a lot of columns (which is why we subsetted to look at only a few of them). Each entry then contains the frequency count for a given token in each document. Of course, this is a *sparse matrix*, meaning that the vast majority of entries are zero (because most SotU speeches will not use every word in the vocabulary--otherwise it would be a long speech!)\n",
    "\n",
    "It's important to note that the order and indices of these documents match the order in our `sotu_df` dataframe from earlier, which contains the text and metadata about each SotU speech. We had better not change the indices otherwise we won't be able to join them later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Searching a Term-Document Matrix\n",
    "\n",
    "It looks like we're in good shape. Now that we have a term-document matrix, we have a metric that we can use when assessing search relevance. We will still filter our `sotu_df` dataframe for those rows that contain our search terms in the `text` column, except now we can join the dataframe with the term frequency counts from our term-document matrix and sort accordingly. Let's see what happens with our earlier search term of \"technology taxes\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num results for query 'technology taxes': 38\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     year president                                               text  \\\n",
       "184  1975      Ford  Mr. Speaker, Mr. Vice President, Members of th...   \n",
       "221  2012     Obama  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "181  1972     Nixon  Mr. Speaker, Mr. President, my colleagues in t...   \n",
       "217  2008      Bush  Madam Speaker, Vice President Cheney, Members ...   \n",
       "213  2004      Bush  Mr. Speaker, Vice President Cheney, Members of...   \n",
       "\n",
       "    technology taxes  terms_sum  \n",
       "184          2    10         12  \n",
       "221          3     8         11  \n",
       "181          6     5         11  \n",
       "217          5     6         11  \n",
       "213          1     9         10  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>president</th>\n      <th>text</th>\n      <th>technology</th>\n      <th>taxes</th>\n      <th>terms_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>184</th>\n      <td>1975</td>\n      <td>Ford</td>\n      <td>Mr. Speaker, Mr. Vice President, Members of th...</td>\n      <td>2</td>\n      <td>10</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>2012</td>\n      <td>Obama</td>\n      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n      <td>3</td>\n      <td>8</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>1972</td>\n      <td>Nixon</td>\n      <td>Mr. Speaker, Mr. President, my colleagues in t...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>2008</td>\n      <td>Bush</td>\n      <td>Madam Speaker, Vice President Cheney, Members ...</td>\n      <td>5</td>\n      <td>6</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>213</th>\n      <td>2004</td>\n      <td>Bush</td>\n      <td>Mr. Speaker, Vice President Cheney, Members of...</td>\n      <td>1</td>\n      <td>9</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# term-document frequency search based on the bag-of-words model\n",
    "def search_td_df(td_df, text_df, query_string: str):\n",
    "    \"\"\"\n",
    "    - Parameters: td_df (Pandas DataFrame) representing a term-document\n",
    "      matrix, text_df (Pandas DataFrame) with a \"text\" column and rows \n",
    "      that correspond to the td_df, and query_string (string).\n",
    "    - Returns: A new dataframe that only contains rows from text_df where\n",
    "      the \"text\" column had at least one occurence of each term in\n",
    "      query_string. Additional columns are added to show the count of\n",
    "      each term and the total count of all terms.\n",
    "    \"\"\"\n",
    "    terms = query_string.lower().split(\" \")\n",
    "    filters = [td_df[term] > 0 for term in terms]\n",
    "    filtered_td_df = td_df[np.all(filters, axis=0)][terms]\n",
    "    filtered_td_df[\"terms_sum\"] = filtered_td_df.agg(sum, axis=1) \\\n",
    "        .astype(\"int64\")\n",
    "    full_df = text_df.merge(filtered_td_df, \n",
    "        left_index=True, right_index=True)\n",
    "\n",
    "    return full_df.sort_values(\"terms_sum\", ascending=False)\n",
    "\n",
    "results = search_td_df(sotu_td_df, sotu_df, search_term)\n",
    "\n",
    "print(f\"Num results for query '{search_term}': {results.shape[0]}\\n\")\n",
    "results.head()"
   ]
  },
  {
   "source": [
    "Already we can see that searching our term-document matrix produced more interesting results than our simple search. We can see term counts for each of our query terms as well as a sum of all the counts together. Given that \"technology\" and \"taxes\" are mentioned more frequently in these documents, they're probably a good place to start.\n",
    "\n",
    "Of course, there are more nuanced ways to look at the search relevance problem. To finish up our introduction to search relevance, we'll look at one more model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Term Frequency-Inverse Document Frequency\n",
    "\n",
    "Term frequency alone is a reasonable metric for search relevance, but imagine a case where you are searching for a term that is relatively common across the entire set of documents. Perhaps the term \"government\" or \"america\" in the case of our corpus of SotU addresses. Nearly all of the documents are likely to mention the terms. Some may use the word \"government\" more often than others, but because of how common the term is across all the documents, it starts to lose its meaning.\n",
    "\n",
    "One way of addressing this problem is the [*term frequency-inverse document frequency*](https://lemmalytica.com/posts/2020/10/11/tf-idf/) (TF-IDF) model. Rather than use term frequency as our metric, we can use TF-IDF to give more weight to rare terms over common terms. We do this by calculating the document frequency (number of documents that include a term at least once) and the inverse document frequency (log of N / document frequency, where N is the total number of documents) and then multiplying it by the term frequency.\n",
    "\n",
    "The result is a measure that still takes into account the number of times a term appears in a document, but gives priority to those terms that are generally rare. Let's take a look at TF-IDF in action.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tf-idf model\n",
    "def document_frequency(td_df, term: str):\n",
    "    \"\"\"\n",
    "    - Parameters: td_df (Pandas DataFrame) representing a term-document\n",
    "      matrix, and term (string).\n",
    "    - Returns: The document frequency value showing the number of\n",
    "      documents in td_df where term occurs at least once.\n",
    "    \"\"\"\n",
    "    return td_df[td_df[term] > 0].shape[0]\n",
    "\n",
    "def inverse_document_frequency(td_df, term: str):\n",
    "    \"\"\"\n",
    "    - Parameters: td_df (Pandas DataFrame) representing a term-document\n",
    "      matrix, and term (string).\n",
    "    - Returns: The inverse document frequency value for term, calculated\n",
    "      as N / log(dft) where N is the number of documents in td_df and\n",
    "      dft is the document frequency value for term.\n",
    "    \"\"\"\n",
    "    N = td_df.shape[0]\n",
    "    dft = document_frequency(td_df, term)\n",
    "    return (N / np.log10(dft))\n",
    "    \n",
    "def build_tfidf_df(td_df):\n",
    "    \"\"\"\n",
    "    - Parameters: td_df (Pandas DataFrame) representing a term-document\n",
    "      matrix.\n",
    "    - Returns: Returns a term frequency-inverse document frequency\n",
    "      (TF-IDF) matrix in the form of a Pandas DataFrame, where each row\n",
    "      is a document and each column is a token. Values in the dataframe\n",
    "      are TF-IDF values for the given document / token.\n",
    "    \"\"\"\n",
    "    def calculate_tfidf(col, td_df):\n",
    "        idf = inverse_document_frequency(td_df, col.name)\n",
    "        return col * idf\n",
    "    \n",
    "    return td_df.apply(calculate_tfidf, td_df=td_df)\n",
    "\n",
    "sotu_tfidf_df = build_tfidf_df(sotu_td_df)"
   ]
  },
  {
   "source": [
    "### Exploring the TF-IDF Matrix\n",
    "\n",
    "Before looking at the TF-IDF matrix, let's see how some IDF scores compare for a relatively common word in the corpus like \"government\" and a rare one like \"moon\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'government' is in 227/228 documents for an IDF of 96.7731314594443.\n'moon' is in 12/228 documents for an IDF of 211.2712770306409.\n"
     ]
    }
   ],
   "source": [
    "def print_idf_info(td_df, word):\n",
    "    num_docs = sotu_td_df.shape[0]\n",
    "    df = document_frequency(td_df, word)\n",
    "    idf = inverse_document_frequency(td_df, word)\n",
    "    print(f\"'{word}' is in {df}/{num_docs} documents with IDF of {idf}.\")\n",
    "\n",
    "print_idf_info(sotu_td_df, \"government\")\n",
    "print_idf_info(sotu_td_df, \"moon\")"
   ]
  },
  {
   "source": [
    "As expected, \"government\" appears in almost every document and thus has a modest IDF score of 96.77, whereas \"moon\" appears in only 12/228 documents and thus has a high IDF score of 211.27. If we were to search our TF-IDF matrix for the terms \"government moon\" then appearances of the term \"moon\" would carry more than twice as much weight as appearances of the term \"government\".\n",
    "\n",
    "Our project is focused on the terms \"technology\" and \"taxes\", neither of which is quite as common as \"government\" or as rare as \"moon\", but the weights will nonetheless differ. Before moving on, let's see what those weights are."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'technology' is in 50/228 documents for an IDF of 134.19895549545362.\n'taxes' is in 131/228 documents for an IDF of 107.68577483094036.\n"
     ]
    }
   ],
   "source": [
    "print_idf_info(sotu_td_df, \"technology\")\n",
    "print_idf_info(sotu_td_df, \"taxes\")"
   ]
  },
  {
   "source": [
    "### Searching the TF-IDF Matrix\n",
    "\n",
    "With the above in mind, let's see how our TF-IDF search differs from the term-document matrix search from earlier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num results for query 'technology taxes': 38\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     year president                                               text  \\\n",
       "184  1975      Ford  Mr. Speaker, Mr. Vice President, Members of th...   \n",
       "181  1972     Nixon  Mr. Speaker, Mr. President, my colleagues in t...   \n",
       "217  2008      Bush  Madam Speaker, Vice President Cheney, Members ...   \n",
       "220  2011     Obama  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "221  2012     Obama  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "    technology    taxes    tfidf_sum  \n",
       "184    268.398  1076.86  1345.255659  \n",
       "181    805.194  538.429  1343.622607  \n",
       "217    670.995  646.115  1317.109426  \n",
       "220    1073.59  215.372  1288.963194  \n",
       "221    402.597  861.486  1264.083065  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>president</th>\n      <th>text</th>\n      <th>technology</th>\n      <th>taxes</th>\n      <th>tfidf_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>184</th>\n      <td>1975</td>\n      <td>Ford</td>\n      <td>Mr. Speaker, Mr. Vice President, Members of th...</td>\n      <td>268.398</td>\n      <td>1076.86</td>\n      <td>1345.255659</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>1972</td>\n      <td>Nixon</td>\n      <td>Mr. Speaker, Mr. President, my colleagues in t...</td>\n      <td>805.194</td>\n      <td>538.429</td>\n      <td>1343.622607</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>2008</td>\n      <td>Bush</td>\n      <td>Madam Speaker, Vice President Cheney, Members ...</td>\n      <td>670.995</td>\n      <td>646.115</td>\n      <td>1317.109426</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>2011</td>\n      <td>Obama</td>\n      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n      <td>1073.59</td>\n      <td>215.372</td>\n      <td>1288.963194</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>2012</td>\n      <td>Obama</td>\n      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n      <td>402.597</td>\n      <td>861.486</td>\n      <td>1264.083065</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# search based on the tf-idf model\n",
    "def search_tfidf_df(tfidf_df, text_df, query_string: str):\n",
    "    \"\"\"\n",
    "    - Parameters: tfidf_df (Pandas DataFrame) representing a tf-idf\n",
    "      matrix, text_df (Pandas DataFrame) with a \"text\" column and rows\n",
    "      that correspond to the tfidf_df, and query_string (string).\n",
    "    - Returns: A new dataframe that only contains rows from text_df where\n",
    "      the corresponding tf-idf value was greater than zero for each of\n",
    "      the terms in query_string. Additional columns are added to show the\n",
    "      tf-idf value for each term and the sum of the tf-idf values. \n",
    "    \"\"\"\n",
    "    terms = query_string.lower().split(\" \")\n",
    "    filters = [tfidf_df[term] > 0 for term in terms]\n",
    "    filtered_tfidf_df = tfidf_df[np.all(filters, axis=0)][terms]\n",
    "    filtered_tfidf_df[\"tfidf_sum\"] = filtered_tfidf_df.agg(sum, axis=1)\n",
    "    full_df = text_df.merge(filtered_tfidf_df, \n",
    "                            left_index=True, right_index=True)\n",
    "\n",
    "    return full_df.sort_values(\"tfidf_sum\", ascending=False)\n",
    "\n",
    "results = search_tfidf_df(sotu_tfidf_df, sotu_df, search_term)\n",
    "\n",
    "print(f\"Num results for query '{search_term}': {results.shape[0]}\\n\")\n",
    "results.head()"
   ]
  },
  {
   "source": [
    "In our TF-IDF model, the word \"technology\" carried about 25% more weight than the word \"taxes\". And indeed, it seems to have affected the results! The rank order of the top five results from the two searches is as follows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  term_frequency_search  tfidf_search\n",
       "0           1975 - Ford   1975 - Ford\n",
       "1          2012 - Obama  1972 - Nixon\n",
       "2          1972 - Nixon   2008 - Bush\n",
       "3           2008 - Bush  2011 - Obama\n",
       "4           2004 - Bush  2012 - Obama"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>term_frequency_search</th>\n      <th>tfidf_search</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1975 - Ford</td>\n      <td>1975 - Ford</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2012 - Obama</td>\n      <td>1972 - Nixon</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1972 - Nixon</td>\n      <td>2008 - Bush</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008 - Bush</td>\n      <td>2011 - Obama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2004 - Bush</td>\n      <td>2012 - Obama</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "td_results = search_td_df(sotu_td_df, sotu_df, search_term) \\\n",
    "    .head(5).reset_index(drop=True)\n",
    "td_results[\"term_frequency_search\"] = td_results[\"year\"].astype(str) \\\n",
    "    + \" - \" + td_results[\"president\"]\n",
    "\n",
    "tfidf_results = search_tfidf_df(sotu_tfidf_df, sotu_df, search_term) \\\n",
    "    .head(5).reset_index(drop=True)\n",
    "tfidf_results[\"tfidf_search\"] = tfidf_results[\"year\"].astype(str) \\\n",
    "    + \" - \" + tfidf_results[\"president\"]\n",
    "\n",
    "top_5 = td_results[[\"term_frequency_search\"]].merge(\n",
    "    tfidf_results[[\"tfidf_search\"]], \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "top_5"
   ]
  },
  {
   "source": [
    "The results may not seem all that different--but imagine that you are working with a significantly larger data set. Weighting your search with TF-IDF may prove decisive in your effort to find the most relevant search results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Challenges\n",
    "\n",
    "Our goal in comparing simple searches, term frequency searches, and TF-IDF searches isn't necessarily to determine which is best. Rather, the goal has been to see how different methods of information retrieval produce different results. Which model you use will depend in part on your goals.\n",
    "\n",
    "Of course, our basic introduction ignores a lot of important issues. For example, should term proximity matter? Should we only return results that include all search terms (as we chose to do here) or should we also return results for partial matches? What about related words? Should a search for \"taxes\" also return results for \"tax\" and \"taxation\"? Or perhaps we shouldn't focus on words at all, but instead focus on themes. Perhaps a search for \"technology\" should also return documents that discuss \"innovation\" and \"science\". The list of possible refinements goes on.\n",
    "\n",
    "Search relevance, and information retrieval more broadly, is a complex and fascinating topic. If you're interested in learning more, [Introduction to Information Retrieval](https://www.amazon.com/gp/product/0521865719/ref=as_li_tl?ie=UTF8&tag=lemmalytica-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0521865719&linkId=f0a6d49537ea9a727debd37dd9e52383) by Christopher D. Manning is arguably the seminal book on the topic and well worth a read.\n",
    "\n",
    "Hopefully this has been an interesting introduction. Stay tuned for more articles on how to explore your text data sets!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}